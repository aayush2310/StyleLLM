{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1d578dc-a439-42f5-8578-6f03791096fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HF Tokenizer\n",
      "Loading HF Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:09<00:00,  4.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF Model Loaded!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteriaList, StoppingCriteria\n",
    "import torch \n",
    "\n",
    "device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "# device = torch.device(\"cpu\")\n",
    "HF_MODEL_PATH = \"tiiuae/falcon-7b-instruct\"\n",
    "print(\"Loading HF Tokenizer\")\n",
    "HF_TOKENIZER = AutoTokenizer.from_pretrained(HF_MODEL_PATH)\n",
    "print(\"Loading HF Model\")\n",
    "HF_MODEL = AutoModelForCausalLM.from_pretrained(HF_MODEL_PATH, trust_remote_code=True)\n",
    "HF_MODEL.to(device)\n",
    "print(\"HF Model Loaded!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e6b9ce1d-4391-4dd6-af3a-bd624bdeef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    def __init__(self, stops=[], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = [stop.to(\"cuda\") for stop in stops]\n",
    "        \n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        for stop in self.stops:\n",
    "            if torch.all((stop == input_ids[0][-len(stop):])).item():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def predict_text_greedy(model, tokenizer, prompt, device, use_cache):\n",
    "    tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = tokenized_prompt.input_ids.to(dtype=torch.long, device=device)\n",
    "    attention_mask = tokenized_prompt.attention_mask.to(dtype=torch.long, device=device)\n",
    "    stop_words = [\"\\n\", \"\\n\\n\"]\n",
    "    stop_words_ids = [\n",
    "        tokenizer(stop_word, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        for stop_word in stop_words\n",
    "        ]\n",
    "    stopping_criteria = StoppingCriteriaList(\n",
    "        [StoppingCriteriaSub(stops=stop_words_ids)]\n",
    "        )\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=50,\n",
    "            use_cache=use_cache,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        preds = [\n",
    "            tokenizer.decode(\n",
    "                g, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "                )\n",
    "                for g in generated_ids\n",
    "                ]\n",
    "    return preds\n",
    "\n",
    "\n",
    "def generateHFModelResponse(user_input, HF_MODEL, HF_TOKENIZER, device):\n",
    "    answer = predict_text_greedy(HF_MODEL, \n",
    "                        HF_TOKENIZER,\n",
    "                        user_input, \n",
    "                        device, \n",
    "                        use_cache=True)[0]\n",
    "    \n",
    "    answer = answer.replace(user_input, '')\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5958cbd1-c71b-4e19-9918-7fb7b9ad23f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"How is the weather?\"\n",
    "\n",
    "generateHFModelResponse(prompt, \n",
    "                       HF_MODEL, \n",
    "                       HF_TOKENIZER,\n",
    "                       device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a609ded1-aaef-419b-9b5d-c12c87648909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Wow, I'm so glad I chose to move to a place like this!\\n\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"The weather here is so bad!\"\n",
    "\n",
    "prompt = f\"\"\"Respond to every input in a sarcastic tone.\n",
    "For example:\n",
    "Input: I think that London is the capital of England?\n",
    "Response: Is it? Wow! You are really intelligent!\n",
    "Input: I only got 15% on my history test.\n",
    "Response: Well done! I'm very impressed!\n",
    "Input: I didn't get the job; I failed the interview.\n",
    "Response: What a surprise!\n",
    "Input: I think you should drive slowly.\n",
    "Response:  Of course, you're the real expert at driving, aren't you?\n",
    "Input: Slow down! You're dancing too fast!\n",
    "Response: Sorry, I forgot you were the expert dancer!\n",
    "Input: {sentence}\n",
    "Response: \"\"\"\n",
    " \n",
    "generateHFModelResponse(prompt, \n",
    "                       HF_MODEL, \n",
    "                       HF_TOKENIZER,\n",
    "                       device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1ca6f7a3-bf8e-4860-9f87-2aef693182e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Wow, I'm sure glad I chose to move to a place like this!\\n\""
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"The weather here is so bad!\"\n",
    "\n",
    "prompt = f\"\"\"Assume you respond to every sentence in a sarcastic tone. Generate a response to the following query the way a sarcastic person would respond.\n",
    "For example:\n",
    "Input: I think that London is the capital of England?\n",
    "Response: Is it? Wow! You are really intelligent!\n",
    "Input: I only got 15% on my history test.\n",
    "Response: Well done! I'm very impressed!\n",
    "Input: I didn't get the job; I failed the interview.\n",
    "Response: What a surprise!\n",
    "Input: I think you should drive slowly.\n",
    "Response:  Of course, you're the real expert at driving, aren't you?\n",
    "Input: Slow down! You're dancing too fast!\n",
    "Response: Sorry, I forgot you were the expert dancer!\n",
    "Input: {sentence}\n",
    "Response: \"\"\"\n",
    " \n",
    "generateHFModelResponse(prompt, \n",
    "                       HF_MODEL, \n",
    "                       HF_TOKENIZER,\n",
    "                       device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6ad24061-5234-4f76-af3c-158dfc2133a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Oh, great. And what will we do when the world is run by large language models? I'm sure it'll be a utopia.\""
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Large language models will take over the world.\"\n",
    "\n",
    "prompt = f\"\"\"Assume you respond to every sentence in a sarcastic tone. Generate a response to the following query the way a sarcastic person would respond.\n",
    "Input: {sentence}\n",
    "Response: (sarcastically)\"\"\"\n",
    "\n",
    "\n",
    "generateHFModelResponse(prompt, \n",
    "                       HF_MODEL, \n",
    "                       HF_TOKENIZER,\n",
    "                       device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "992d3a77-93ad-4058-9aa9-fb146fc9c10b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Oh, great. And what will we do when the world is run by large language models? I'm sure it'll be a utopia.\""
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Large language models will take over the world.\"\n",
    "\n",
    "prompt = f\"\"\"Assume you are respond to every sentence in a sarcastic tone. Generate a response to the following query the way a sarcastic person would respond.\n",
    "Input: {sentence}\n",
    "Response: (sarcastically)\"\"\"\n",
    "\n",
    "\n",
    "generateHFModelResponse(prompt, \n",
    "                       HF_MODEL, \n",
    "                       HF_TOKENIZER,\n",
    "                       device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c4064909-6b89-4b06-9d68-d3b9af4d89c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Oh, I'm sure they wouldn't. Large Language Models are just a bunch of nerdy linguistics types who spend their lives studying language. They'd probably be more interested in analyzing the nuances of human speech than taking over the world\""
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Assume you are Chandler Bing from Friends TV Series. Generate a response to the following query the way Chandler would respond.\n",
    "Joey: Do you think Large Language Models would take-over the world?\n",
    "Chandler: (sarcastically)\"\"\"\n",
    "\n",
    "generateHFModelResponse(prompt, \n",
    "                       HF_MODEL, \n",
    "                       HF_TOKENIZER,\n",
    "                       device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bff8643-93c2-413f-bae7-377c884073a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stylebam",
   "language": "python",
   "name": "stylebam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
